---
layout: post
title: "Python爬虫实践"
date: 2017-03-22
comments: true
categories: Python
tags: [Python]
keywords: Python爬虫实践
publish: true
description:  Python爬虫实践
---
>看了[廖雪峰老师的Python教程](http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000)觉得非常的棒，浅显易懂，知识丰富，很适合初学者入门。看了一段时间，决定写点小东西练习一下，于是，就打算使用Python抓取一下廖雪峰老师的Python教程😏。

<!-- more -->
## 前言
打开廖雪峰老师的Python3教程，由于是个人博客，内容也十分简洁，而我们需要抓取的内容也很容易找到。工欲善其事必先利其器，先安装好Python3和PyCharm开发工具，这里就不在赘述了。
## 分析

我们先打开这篇教程，就是这个：

![教程首页](http://om6homgqk.bkt.clouddn.com/教程首页.png)

在抓取前我们先看一下网页的源码，找到我们需要抓取的标签。如图：

![网页源码](http://om6homgqk.bkt.clouddn.com/网页源码.png)

不难发现，上图的`<ul>`标签就是我们需要抓取的目标，也就是网页左侧的目录，我们需要的文章链接和标题都在这里。
OK！现在万事具备了，大致可以指定以下流程了：

```
1. 获取整个html的内容；
2. 从html中匹配出我们所需的文章链接及标题；
3. 下载整个html或者是保存为PDF
```
大致就是这个样子吧，至于步骤三如果下载html的话还需要下载整个页面的css文件，这里先只下载html吧。
## 开始
为了完成步骤1，我们先编写一个根据url地址，获取网页内容的函数：

```
def url_open(url):
    url = re.sub(r'^//*', 'http://', url)
    html = urllib.request.urlopen(url)
    return html.read()
```
`re`是Python内置的处理正则的模块，上面的代码把以`//`开头的链接替换为`http://`开头的完整url，并读取整个html的内容。
之后我们就可以在整个页面的内容中寻找我们所需要的内容了，这并没有什么难度，关键是如何利用好正则。

```
# 获取文章标题和链接
def find_itemlist(url):
    html = url_open(url).decode('utf-8')
    soup = BeautifulSoup(html)
    # 获取<ul>标签内容
    ul = soup.find_all('ul', class_='uk-nav uk-nav-side', style="margin-right:-15px;")
    print(ul)
    if len(ul):
        ulstr = str(ul[0])
    # 获取<a>标签内容
    pattern = r'<a.*>.*</a>'
    li = re.findall(pattern, ulstr)
    items = []
    for a in li:
        # 获取href标签内容
        hrefs = re.findall(r'href=".*"', a)
        if len(hrefs):
            href = hrefs[0]
            href = href[6:-1]
            print(href)
        # 获取文章标题
        titles = re.findall(r'>.*</a>', a)
        if len(titles):
            title = titles[0]
            title = title[1:-4]
            print(title)
        items.append({'title':title, 'href':href})
    return items
```
原谅我渣渣的正则水平，我先使用Python的BeautifulSoup模块获取了`<ul>...</ul>`标签内容，然后又从中获取到了`<a>`标签内容，最后从`<a>`标签中获取`href`标签内容和文章的标题。现在我们有了一个装有文章标题和链接的数组，接下来想干嘛就可以干嘛了😏。

```
# 保存html文件
def save_html(url, file_name):
    html = url_open(url)
    # 注意文件名称不能带有斜杠，需要用其他字符串代替。
    with open(file_name.replace('/','_') + '.html', 'wb') as f:
        try:
            f.write(html)
            print(file_name + '保存完成')
        except:
            print(file_name + '写入失败')
            
# 遍历数组，开始写入文件             
def download(items):
    # 创建根文件夹
    root = 'XXX'
    if os.path.exists(root):
        os.remove(root)
    os.mkdir(root)
    os.chdir(root)
    basurl = 'http://www.liaoxuefeng.com'
    for item in items:
        url = item['href']
        url = basurl + url
        file_name = item['title']
        save_html(url, file_name,)
    os.close()
    print("全部文章共%s篇,保存完成!" % len(items))

```
至此，我们的小爬虫就算完成了，虽然很简单，但也是一个好的开始，如果你有兴趣可以吧所有内容转换成PDF保存，我尝试了第三方生成PDF的库`pdfkit`，转成PDF的效果并不是很好，代码其实很简单，如下：

```
def save_pdf(url, file_name):
    try:
        pdfkit.from_url(url, file_name.replace('/','_') + '.pdf')
        print(file_name + '保存完成')
    except:
        print(file_name + '写入失败')
```
## 最后
其实，这里写入文件的操作用多线程效率会更加的高一些，留着下次尝试吧，本篇结束😝。

